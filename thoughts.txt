## when running models:

# Set all 8 GPUs to be visible
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7

# Start SGLang with optimal settings for H200
python -m sglang.launch_server \
  --model-path meta-llama/Llama-3.2-11B-Vision-Instruct \
  --host 127.0.0.1 \
  --port 11434 \
  --tensor-parallel-size 1 \
  --tool-call-parser llama3 \
  --enable-multimodal \
  --trust-remote-code \
  --mem-fraction-static 0.9 \
  --max-total-tokens 32768

# To start with vllm
python -m vllm.entrypoints.openai.api_server   --model Qwen/Qwen2.5-VL-7B-Instruct   --host 127.0.0.1   --port 11434   --tensor-parallel-size 1 --enable-auto-tool-choice --tool-call-parser hermes --chat-template ./chat_template.jinja

Models available : https://github.com/eugeneyan/open-llms?utm_source=chatgpt.com

# Regarding multi-processing issue:

1. run 1 pool worker only - will take a lot of time
2. get access to 8 GPUs and run a rag worker on all 8 - ingestion/addition will create problem, unless we make it 
    read/retrieve only and pre-built the database 
3. creating a separate write_only process which writes into the DB, all the other rag workes submit request to this process.
    - problem is we do web-search store results, and then call retrieve again 
    - what if we process and extract data from that results and go ahead, and let the write-process write it later?
      - will be doing the same work twice
    - we can use the chroma server mode using HttpClient(host="", port=) instead of PersistentClient(path="") to solve the issue
    of stale data. That is, write is adding the data but reader doesn't see new data
      - 
future advancements:

- creating agents for individual function : evaluation, retrieval etc. (multi-agent architecture)
- using the custom databases developed by Tushar
- using the questions developed by Tushar for keyword extraction